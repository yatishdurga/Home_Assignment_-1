# Home_Assignment_-1
# TensorFlow Experiments in Google Colab

## ğŸ“Œ Overview
This repository contains TensorFlow-based implementations of key deep learning concepts, including tensor manipulations, loss functions, optimizers, and training a neural network with TensorBoard. The experiments are run in **Google Colab**.

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ Tensor_Manipulations.ipynb      # Tensor reshaping and operations
â”œâ”€â”€ Loss_Functions_Tuning.ipynb     # Comparing loss functions
â”œâ”€â”€ Optimizers_Comparison.ipynb     # Training with Adam vs. SGD
â”œâ”€â”€ Neural_Network_TensorBoard.ipynb # Training and logging with TensorBoard
â”œâ”€â”€ README.md                        # Project Documentation
```

## ğŸ”¥ 1. Tensor Manipulations & Reshaping
### âœ… Tasks:
- Create a random tensor of shape **(4,6)**.
- Find its **rank and shape**.
- Reshape it into **(2,3,4)** and then **transpose** it to **(3,2,4)**.
- Broadcast a smaller tensor **(1,4)** and add it to the larger tensor.
- Explain **broadcasting in TensorFlow**.

### ğŸ“Œ Expected Output:
- Print tensor **rank and shape** before and after reshaping/transposing.

ğŸ“– **Concepts Covered:** Tensor Operations, Reshaping, Transposing, Broadcasting

---

## ğŸ¯ 2. Loss Functions & Hyperparameter Tuning
### âœ… Tasks:
- Define **true values (y_true)** and **model predictions (y_pred)**.
- Compute:
  - **Mean Squared Error (MSE)** â€“ Used in regression.
  - **Categorical Cross-Entropy (CCE)** â€“ Used in classification.
- Modify predictions and observe **loss changes**.
- Plot **MSE vs. CCE loss values** using Matplotlib.

### ğŸ“Œ Expected Output:
- Loss values printed for different predictions.
- **Bar chart comparing MSE and CCE loss.**

ğŸ“– **Concepts Covered:** Loss Functions, Model Performance Metrics

---

## âš¡ 3. Train a Model with Different Optimizers
### âœ… Tasks:
- Load the **MNIST dataset**.
- Train two models:
  - One with **Adam Optimizer**.
  - One with **SGD Optimizer**.
- Compare **training and validation accuracy trends**.

### ğŸ“Œ Expected Output:
- Accuracy plots comparing **Adam vs. SGD performance**.

ğŸ“– **Concepts Covered:** Optimizers, Model Training, Accuracy Comparison

---

## ğŸ“Š 4. Train a Neural Network and Log to TensorBoard
### âœ… Tasks:
- Load the **MNIST dataset** and preprocess it.
- Train a **simple neural network** with TensorBoard logging.
- Launch **TensorBoard in Google Colab**.
- Analyze **training vs. validation accuracy and loss** trends.

### ğŸ“Œ Expected Output:
- Model trains for **5 epochs**, logs stored in `logs/fit/`.
- Visualization of **training vs. validation accuracy and loss** in TensorBoard.

ğŸ“– **Concepts Covered:** Deep Learning Model Training, TensorBoard, Visualization

---

## ğŸ” 4.1 Questions Answered
1ï¸âƒ£ **What patterns do we observe in accuracy curves?**
   - Training accuracy **increases**, but validation accuracy may **plateau or drop** due to overfitting.

2ï¸âƒ£ **How to detect overfitting with TensorBoard?**
   - If **training loss keeps decreasing** but **validation loss increases**, the model is overfitting.
   - Solutions: **Dropout, L2 Regularization, Data Augmentation**.

3ï¸âƒ£ **What happens if we increase epochs?**
   - Initially, **both accuracies improve**.
   - After too many epochs, the model **overfits** â€“ training accuracy remains high, but validation accuracy drops.
   - Solutions: **Early Stopping, Reduce Learning Rate**.

---

## ğŸ› ï¸ How to Run the Code in Google Colab
1. Open **Google Colab**.
2. Upload the notebooks from this repository.
3. Run the cells step by step.
4. For TensorBoard visualization, use:
   ```python
   %load_ext tensorboard
   %tensorboard --logdir logs/fit/
   ```

